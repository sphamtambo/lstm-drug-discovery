{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7451242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 41127\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>activity</th>\n",
       "      <th>HIV_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles activity  HIV_active\n",
       "0  CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...       CI           0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/HIV.csv\")\n",
    "print(f\"size: {df.shape[0]}\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364dbf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32901.6], [4112.7], [4112.7])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "[0.8 *len(df)], [0.1 *len(df)], [0.1 *len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642499d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/HIV_smiles.npy\", np.array(df.smiles), fmt='%s')\n",
    "np.savetxt(\"../../data/HIV_labels.npy\", np.array(df.HIV_active), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d10908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(smiles):\n",
    "    ## https://github.com/topazape/LSTM_Chem/blob/master/lstm_chem/utils/smiles_tokenizer2.py\n",
    "\n",
    "    atoms = [\n",
    "        'Al', 'As', 'B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'K', 'Li', 'N',\n",
    "        'Na', 'O', 'P', 'S', 'Se', 'Si', 'Te'\n",
    "    ]\n",
    "    special = [\n",
    "        '(', ')', '[', ']', '=', '#', '%', '0', '1', '2', '3', '4', '5',\n",
    "        '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
    "    ]\n",
    "    padding = ['<eos>', '<sos>', '<pad>']\n",
    "\n",
    "    vocab = sorted(atoms, key=len, reverse=True) + special + padding\n",
    "\n",
    "    table_2_chars = list(filter(lambda x: len(x) == 2, vocab))\n",
    "    table_1_chars = list(filter(lambda x: len(x) == 1, vocab))\n",
    "\n",
    "    smiles = smiles + ' '\n",
    "    N = len(smiles)\n",
    "    token = []\n",
    "    i = 0\n",
    "    while (i < N):\n",
    "        c1 = smiles[i]\n",
    "        c2 = smiles[i:i + 2]\n",
    "\n",
    "        if c2 in table_2_chars:\n",
    "            token.append(c2)\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        if c1 in table_1_chars:\n",
    "            token.append(c1)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    vocab_dict = {}\n",
    "    for i, tok in enumerate(vocab):\n",
    "        vocab_dict[tok] = i\n",
    "\n",
    "    encoded = [vocab_dict[t] for t in token]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2a3f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7fa6837131407395e1f61ec5571f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_ids = []\n",
    "    for line in tqdm(df.smiles):\n",
    "        ids = tokenizer(line)\n",
    "        input_ids.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49d604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41127"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f5a4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dirs\n",
    "smiles_dir = \"../../data/HIV_smiles.npy\"\n",
    "labels_dir = \"../../data/HIV_labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32927acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label, tokenizer):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.data = np.loadtxt(data, dtype=object)    \n",
    "        self.label = np.loadtxt(label, dtype=float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "        input_id = tokenizer(text)\n",
    "        return torch.tensor(input_id, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ff9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = CustomDataset(smiles_dir, labels_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa34668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32901.6], [4112.7], [4112.7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "[0.8 *len(dataset)], [0.1 *len(dataset)], [0.1 *len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d62ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [32903, 4112, 4112]) # 80, 10, 10\n",
    "# len(train_dataset + test_dataset + val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf5f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID:\n",
      "  tensor([10, 18, 40, 27, 41, 40, 19, 15, 20, 40, 19, 15, 10, 28, 16, 10, 19, 10,\n",
      "        16, 20, 10, 19, 16, 20, 10, 28, 16, 20, 40, 19, 16, 20, 41, 27])\n",
      "Label:\n",
      " tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "for smiles, labels in train_dataset:\n",
    "    print(\"Input ID:\\n \" ,smiles)\n",
    "    print(\"Label:\\n\" ,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b0225ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for data, label in batch:\n",
    "        ## smiles (copy smiles without...)\n",
    "        processed_text = data.clone().detach() \n",
    "        text_list.append(processed_text)\n",
    "        ## label\n",
    "        label_list.append(label)\n",
    "        ## length\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "        ## padding\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "                       \n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b03a1adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10, 16, 40,  ...,  0,  0,  0],\n",
       "         [15,  0,  0,  ...,  0,  0,  0],\n",
       "         [16, 23, 18,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [10, 16, 40,  ...,  0,  0,  0],\n",
       "         [10, 16, 40,  ...,  0,  0,  0],\n",
       "         [10, 10, 10,  ...,  0,  0,  0]]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([74,  1, 51,  ..., 49, 54, 35]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b449784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4,shuffle=False, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4,shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4,shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "306151be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 18, 40, 27, 41, 40, 19, 15, 20, 40, 19, 15, 10, 28, 16, 10, 19, 10,\n",
      "         16, 20, 10, 19, 16, 20, 10, 28, 16, 20, 40, 19, 16, 20, 41, 27,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 10, 16, 10, 19, 23, 16, 20, 10, 19, 23, 10, 15, 10, 19, 23, 18, 20,\n",
      "         15, 40, 27, 40, 40, 19, 10, 20, 40, 40, 40, 27, 10, 20, 10, 19, 23, 16,\n",
      "         20, 16, 10, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 16, 10, 19, 23, 16, 20, 10, 19, 10, 20, 15, 10, 19, 23, 16, 20, 10,\n",
      "         19, 10, 10, 18, 10, 20, 15, 10, 19, 23, 16, 20, 10, 19, 10, 40, 27, 40,\n",
      "         40, 40, 40, 40, 27, 20, 15, 10, 19, 23, 16, 20, 16, 10, 19, 10, 20, 19,\n",
      "         10, 20, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 10, 19, 23, 16, 20, 16, 40, 27, 40, 40, 28, 40, 19, 40, 40, 27, 16,\n",
      "         10, 19, 10, 20, 23, 16, 20, 10, 27, 19, 10, 20, 10, 10, 10, 28, 19, 10,\n",
      "         20, 40, 28, 40, 40, 19, 16, 10, 19, 10, 20, 23, 16, 20, 40, 19, 16, 10,\n",
      "         19, 10, 20, 23, 16, 20, 40, 40, 28, 27]])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(train_dataloader))\n",
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "832b557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edffeddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([34, 40, 57, 64])\n"
     ]
    }
   ],
   "source": [
    "print(length_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e84fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a48af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## building an rnn model\n",
    "## many to one classificaton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0b8404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size,\n",
    "                 num_layers, fc_hidden_size, output_size):\n",
    "        super(). __init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim (embedding))\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, num_layers, batch_first=True) \n",
    "        # only use the hidden_size, since its a many to one\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # output_size, use 1 for binary classification\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, output_size) \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        # text_length = [batch size]\n",
    "        \n",
    "        # [sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        # pack sequence to avoid using <paddings> during computations (saves computations)\n",
    "        # lengths needs to be on cpu\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, text_lengths.cpu().numpy(),\n",
    "                                                enforce_sorted=False, batch_first=True)\n",
    "        # =>  [batch_size, sentence len,  embedding dim]\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        out, (hidden, cell) = self.rnn(out) ## lstm with input, hidden, and internal (cell) state\n",
    "        # out dim: [batch size, sentence length, hidden dim]\n",
    "        # cell dim: [num layers, batch size, hidden dim]\n",
    "        # hidden dim: [num_layers, batch_size, hidden dim]\n",
    "        \n",
    "        # use final hidden state from the last layer as an input to fc1\n",
    "        # Index hidden state of last time step\n",
    "        # hidden.size() --> [num_layers, batch_size, hidden dim]\n",
    "        out = hidden[-1, :, :]  \n",
    "        # new out.size() --> batch_size, hidden dim\n",
    "        out = self.fc1(out) ## first dense layer\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out) ## final layer\n",
    "        out = self.sigmoid(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a426fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 60 ## len of vocab size\n",
    "embed_dim = 70 ## input size, usually around 50-500 dimensions\n",
    "num_layers = 1 ## number of recurrent layers, 2 would mean stacking 2 layers to form stacked LSTM\n",
    "rnn_hidden_size = 100 ## usually around 100-500 dimensions\n",
    "fc_hidden_size = 100 ## usually around 100-500 dimensions\n",
    "output_size = 1 ## since the output is between 0 and 1, thus 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90638f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will instantiate the class LSTM1 object.\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size,\n",
    "            num_layers, fc_hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91feee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(60, 70, padding_idx=0)\n",
       "  (rnn): LSTM(70, 100, batch_first=True)\n",
       "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f744f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    # model training mode (gradient computation)\n",
    "    model.train()\n",
    "    # initailiz acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    for idx, (text, label, length) in enumerate (dataloader):\n",
    "        # reset gradients to zero before each instance\n",
    "        optimizer.zero_grad()\n",
    "        # label predictions (forward papagation)\n",
    "        # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "        predicted_label = model(text, length).squeeze(1) # or [:,0]\n",
    "        # loss calculation\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        # compute gradients (backward propagation) \n",
    "        # to minimize loss functions with gradient descent\n",
    "        loss.backward()\n",
    "        # update parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "        ## logging\n",
    "        if not idx % 1500:\n",
    "            print('| Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f'\n",
    "                  %(epoch +1, num_epochs, idx, len(dataloader), loss))\n",
    "        # compute total accuracy\n",
    "        # if pred label is >=0.5 to probability of true truth accuracy count increases\n",
    "        # summation of the largest value which yields predicted class label\n",
    "        total_acc += ((predicted_label >= 0.5).float() == label).float().sum().item()\n",
    "        # compute total loss after back prop and parameter update\n",
    "        total_loss += loss.item() * label.size(0)\n",
    "    # compare true labels with the predicted labels to compute accuracy\n",
    "    return total_acc/len(dataloader.dataset), \\\n",
    "            total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5b6545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    # model evaluation mode (no gradient computation)\n",
    "    model.eval()\n",
    "    # initailize acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    # disabling gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for text, label, length in dataloader:\n",
    "            # label predictions (forward papagation)\n",
    "            # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "            predicted_label = model(text, length).squeeze(1) # reshape\n",
    "            # loss calculation\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            # compute total accuracy\n",
    "            # if pred label is >=0.5 to probability of true truth accuracy count increases\n",
    "            # summation of the largest value which yields predicted class label\n",
    "            total_acc += ((predicted_label >= 0.5).float() == label).float().sum().item()\n",
    "            # compute total loss after back prop and parameter update\n",
    "            total_loss += loss.item() * label.size(0)\n",
    "            # compare true labels with the predicted labels to compute accuracy\n",
    "        # compare true labels with the predicted labels to compute accuracy\n",
    "        return total_acc/len(dataloader.dataset), \\\n",
    "                total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ef067ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binnary classification we use\n",
    "# single class membership probability output (binary cross entropy loss)\n",
    "loss_fn = nn.BCELoss()\n",
    "# Adam Optimizer to update parameters based on the computed gradients\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c632c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 001/003 | Batch 000/8226 | Loss: 0.0043\n",
      "| Epoch: 001/003 | Batch 1500/8226 | Loss: 0.0303\n",
      "| Epoch: 001/003 | Batch 3000/8226 | Loss: 0.0505\n",
      "| Epoch: 001/003 | Batch 4500/8226 | Loss: 0.0167\n",
      "| Epoch: 001/003 | Batch 6000/8226 | Loss: 0.0315\n",
      "| Epoch: 001/003 | Batch 7500/8226 | Loss: 0.0291\n",
      "| Epoch: 001/003 | Train Acc: 96.52% | Valid. Acc: 96.64%\n",
      "| Time elapsed: 4.88 min\n",
      "| Epoch: 002/003 | Batch 000/8226 | Loss: 0.0369\n",
      "| Epoch: 002/003 | Batch 1500/8226 | Loss: 0.0259\n",
      "| Epoch: 002/003 | Batch 3000/8226 | Loss: 0.0477\n",
      "| Epoch: 002/003 | Batch 4500/8226 | Loss: 0.0265\n",
      "| Epoch: 002/003 | Batch 6000/8226 | Loss: 0.0268\n",
      "| Epoch: 002/003 | Batch 7500/8226 | Loss: 0.0392\n",
      "| Epoch: 002/003 | Train Acc: 96.53% | Valid. Acc: 96.64%\n",
      "| Time elapsed: 9.58 min\n",
      "| Epoch: 003/003 | Batch 000/8226 | Loss: 0.0536\n",
      "| Epoch: 003/003 | Batch 1500/8226 | Loss: 0.0228\n",
      "| Epoch: 003/003 | Batch 3000/8226 | Loss: 0.0311\n",
      "| Epoch: 003/003 | Batch 4500/8226 | Loss: 0.0146\n",
      "| Epoch: 003/003 | Batch 6000/8226 | Loss: 0.0253\n",
      "| Epoch: 003/003 | Batch 7500/8226 | Loss: 0.0341\n",
      "| Epoch: 003/003 | Train Acc: 96.57% | Valid. Acc: 96.74%\n",
      "| Time elapsed: 14.15 min\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 3\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dataloader)\n",
    "    acc_val , loss_val = evaluate(val_dataloader)\n",
    "    print('| Epoch: %03d/%03d | Train Acc: %.2f%% | Valid. Acc: %.2f%%'\n",
    "          %(epoch + 1, num_epochs, 100 * acc_train, 100 * acc_val))\n",
    "    print(f'| Time elapsed: {(time.time() - start_time) / 60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d524686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 96.06\n"
     ]
    }
   ],
   "source": [
    "acc_test, _ = evaluate(test_dataloader)\n",
    "print('Test Acc: %.2f'\n",
    "      %(100 * acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
