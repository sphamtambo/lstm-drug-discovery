{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417bc3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch. __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec212d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047f802b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7451242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 2593\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>IC50</th>\n",
       "      <th>units</th>\n",
       "      <th>activity</th>\n",
       "      <th>pIC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>CHEMBL203661</td>\n",
       "      <td>O=C(C#CCCN1CCOCC1)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncn...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>nM</td>\n",
       "      <td>high</td>\n",
       "      <td>8.903090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>CHEMBL203599</td>\n",
       "      <td>CN1CCN(CCC#CC(=O)Nc2cc3c(Nc4ccc(F)c(Cl)c4)ncnc...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>nM</td>\n",
       "      <td>high</td>\n",
       "      <td>8.845098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>CHEMBL204638</td>\n",
       "      <td>O=C(C#CCCN1CCOCC1)Nc1cc2c(Nc3ccc(F)c(Br)c3)ncn...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>nM</td>\n",
       "      <td>high</td>\n",
       "      <td>8.845098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>CHEMBL3344216</td>\n",
       "      <td>CN(C)[C@H](CS(C)(=O)=O)c1ccc(-c2ccc3ncnc(Nc4cc...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>nM</td>\n",
       "      <td>high</td>\n",
       "      <td>8.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>CHEMBL437890</td>\n",
       "      <td>CN1CCN(CCC#CC(=O)Nc2cc3c(Nc4ccc(F)c(Br)c4)ncnc...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>nM</td>\n",
       "      <td>high</td>\n",
       "      <td>8.698970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                                             smiles  IC50  \\\n",
       "2588   CHEMBL203661  O=C(C#CCCN1CCOCC1)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncn...   0.8   \n",
       "2589   CHEMBL203599  CN1CCN(CCC#CC(=O)Nc2cc3c(Nc4ccc(F)c(Cl)c4)ncnc...   0.7   \n",
       "2590   CHEMBL204638  O=C(C#CCCN1CCOCC1)Nc1cc2c(Nc3ccc(F)c(Br)c3)ncn...   0.7   \n",
       "2591  CHEMBL3344216  CN(C)[C@H](CS(C)(=O)=O)c1ccc(-c2ccc3ncnc(Nc4cc...   0.5   \n",
       "2592   CHEMBL437890  CN1CCN(CCC#CC(=O)Nc2cc3c(Nc4ccc(F)c(Br)c4)ncnc...   0.5   \n",
       "\n",
       "     units activity     pIC50  \n",
       "2588    nM     high  8.903090  \n",
       "2589    nM     high  8.845098  \n",
       "2590    nM     high  8.845098  \n",
       "2591    nM     high  8.698970  \n",
       "2592    nM     high  8.698970  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/her_molecules.csv\")\n",
    "print(f\"size: {df.shape[0]}\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96f10a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['low', 'medium', 'high'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.activity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620e666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       1097\n",
       "high      1048\n",
       "medium     448\n",
       "Name: activity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba5bfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e48abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = CustomDataset(df.smiles, df.activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa34668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2074.4], [259.3], [259.3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "[0.8 *len(dataset)], [0.1 *len(dataset)], [0.1 *len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30d62ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2593"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [2075, 259, 259]) # 80, 10, 10\n",
    "len(train_dataset + test_dataset + val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcf5f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID:\n",
      "  COc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OC[C@@H]1CN2CCC[C@@H]2CO1\n",
      "Label:\n",
      " high\n"
     ]
    }
   ],
   "source": [
    "for smiles, labels in train_dataset:\n",
    "    print(\"Input ID:\\n \" ,smiles)\n",
    "    print(\"Label:\\n\" ,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37109c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/topazape/LSTM_Chem/blob/master/lstm_chem/utils/smiles_tokenizer2.py\n",
    "\n",
    "class SmilesTokenizer(object):\n",
    "    def __init__(self):\n",
    "        atoms = [\n",
    "            'Al', 'As', 'B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'K', 'Li', 'N',\n",
    "            'Na', 'O', 'P', 'S', 'Se', 'Si', 'Te'\n",
    "        ]\n",
    "        special = [\n",
    "            '(', ')', '[', ']', '=', '#', '%', '0', '1', '2', '3', '4', '5',\n",
    "            '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
    "        ]\n",
    "\n",
    "        self.table = sorted(atoms, key=len, reverse=True) + special \n",
    "\n",
    "        self.table_2_chars = list(filter(lambda x: len(x) == 2, self.table))\n",
    "        self.table_1_chars = list(filter(lambda x: len(x) == 1, self.table))\n",
    "        self.vocab_dict = {}\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "        smiles = smiles + ' '\n",
    "        N = len(smiles)\n",
    "        token = []\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            c1 = smiles[i]\n",
    "            c2 = smiles[i:i + 2]\n",
    "\n",
    "            if c2 in self.table_2_chars:\n",
    "                token.append(c2)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            if c1 in self.table_1_chars:\n",
    "                token.append(c1)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return np.asarray(token, dtype=object)\n",
    "        \n",
    "    def vocaburaly(self):\n",
    "        vocab_dict = {}\n",
    "        for i, tok in enumerate(self.table):\n",
    "            vocab_dict[tok] = i\n",
    "        return vocab_dict\n",
    "    \n",
    "    def index_encode(self, tokenized_smiles):\n",
    "        vocab_dict = {}\n",
    "        for i, tok in enumerate(self.table):\n",
    "            vocab_dict[tok] = i\n",
    "        encoded = [vocab_dict[t] for t in tokenized_smiles ]\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4632b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SmilesTokenizer()\n",
    "tokens = [tokenizer.tokenize(x) for x in df.smiles]\n",
    "vocabulary = tokenizer.vocaburaly()\n",
    "indexed_smiles = [tokenizer.index_encode(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf898660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Al': 0,\n",
       " 'As': 1,\n",
       " 'Br': 2,\n",
       " 'Cl': 3,\n",
       " 'Li': 4,\n",
       " 'Na': 5,\n",
       " 'Se': 6,\n",
       " 'Si': 7,\n",
       " 'Te': 8,\n",
       " 'B': 9,\n",
       " 'C': 10,\n",
       " 'F': 11,\n",
       " 'H': 12,\n",
       " 'I': 13,\n",
       " 'K': 14,\n",
       " 'N': 15,\n",
       " 'O': 16,\n",
       " 'P': 17,\n",
       " 'S': 18,\n",
       " '(': 19,\n",
       " ')': 20,\n",
       " '[': 21,\n",
       " ']': 22,\n",
       " '=': 23,\n",
       " '#': 24,\n",
       " '%': 25,\n",
       " '0': 26,\n",
       " '1': 27,\n",
       " '2': 28,\n",
       " '3': 29,\n",
       " '4': 30,\n",
       " '5': 31,\n",
       " '6': 32,\n",
       " '7': 33,\n",
       " '8': 34,\n",
       " '9': 35,\n",
       " '+': 36,\n",
       " '-': 37,\n",
       " 'se': 38,\n",
       " 'te': 39,\n",
       " 'c': 40,\n",
       " 'n': 41,\n",
       " 'o': 42,\n",
       " 's': 43}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e9ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab1 = vocab(ordered_dict)\n",
    "vocab1.insert_token(\"<pad>\", 0)\n",
    "vocab1.insert_token(\"<unk>\", 1)\n",
    "vocab1.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6277d32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('s', 43),\n",
       "             ('o', 42),\n",
       "             ('n', 41),\n",
       "             ('c', 40),\n",
       "             ('te', 39),\n",
       "             ('se', 38),\n",
       "             ('-', 37),\n",
       "             ('+', 36),\n",
       "             ('9', 35),\n",
       "             ('8', 34),\n",
       "             ('7', 33),\n",
       "             ('6', 32),\n",
       "             ('5', 31),\n",
       "             ('4', 30),\n",
       "             ('3', 29),\n",
       "             ('2', 28),\n",
       "             ('1', 27),\n",
       "             ('0', 26),\n",
       "             ('%', 25),\n",
       "             ('#', 24),\n",
       "             ('=', 23),\n",
       "             (']', 22),\n",
       "             ('[', 21),\n",
       "             (')', 20),\n",
       "             ('(', 19),\n",
       "             ('S', 18),\n",
       "             ('P', 17),\n",
       "             ('O', 16),\n",
       "             ('N', 15),\n",
       "             ('K', 14),\n",
       "             ('I', 13),\n",
       "             ('H', 12),\n",
       "             ('F', 11),\n",
       "             ('C', 10),\n",
       "             ('B', 9),\n",
       "             ('Te', 8),\n",
       "             ('Si', 7),\n",
       "             ('Se', 6),\n",
       "             ('Na', 5),\n",
       "             ('Li', 4),\n",
       "             ('Cl', 3),\n",
       "             ('Br', 2),\n",
       "             ('As', 1),\n",
       "             ('Al', 0)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ed6b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 43), ('o', 42), ('n', 41), ('c', 40), ('te', 39), ('se', 38), ('-', 37)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_freq_tuples[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2334133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', 's', 'o', 'n', 'c', 'te', 'se', '-', '+']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59bd161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(vocab1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d691b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (text, label) in train_dataset]))\n",
    "print(f\"Number of classes: {num_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c80a36c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 22, 27, 26, 22, 29, 25]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: [vocab1[token] for token in tokenizer.tokenize(x)]\n",
    "text_pipeline(\"O=S(=O)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28c4e777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline = lambda x: 0 if x == \"low\" else (1 if x == \"medium\" else 2)\n",
    "label_pipeline(\"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "074984fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for   _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text),\n",
    "                                      dtype=torch.long)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list,\n",
    "                                 dtype=torch.long)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
    "                                                     batch_first=True)  \n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24a8018f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35, 35, 26,  ...,  0,  0,  0],\n",
       "         [35, 29,  5,  ...,  0,  0,  0],\n",
       "         [30, 35, 26,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [29, 22, 35,  ...,  0,  0,  0],\n",
       "         [35, 27, 26,  ...,  0,  0,  0],\n",
       "         [29, 22, 35,  ...,  0,  0,  0]]),\n",
       " tensor([2, 2, 2, 0, 0, 2, 0, 0, 2, 1, 2, 0, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 1, 2,\n",
       "         2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0,\n",
       "         0, 2, 0, 2, 2, 0, 2, 2, 2, 1, 2, 2, 0, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0,\n",
       "         1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0,\n",
       "         2, 0, 1, 2, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0,\n",
       "         2, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2,\n",
       "         1, 2, 0, 0, 2, 0, 2, 0, 1, 2, 2, 1, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2, 2, 2,\n",
       "         2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 1, 0, 0, 2, 2,\n",
       "         2, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 2, 2, 0, 1, 0, 0, 2, 1, 2, 2,\n",
       "         2, 0, 1, 0, 1, 1, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 0, 2, 1, 0, 0, 0, 2,\n",
       "         2, 0, 0, 0, 1, 2, 1, 0, 2, 1, 2, 1, 0, 0, 1, 0, 0, 2, 0]),\n",
       " tensor([56, 72, 56, 32, 62, 61, 47, 58, 68, 58, 59, 39, 60, 42, 55, 41, 59, 45,\n",
       "         62, 91, 45, 37, 49, 62, 39, 59, 59, 41, 68, 73, 53, 93, 35, 93, 69, 45,\n",
       "         62, 71, 75, 55, 42, 37, 64, 50, 63, 65, 77, 25, 53, 64, 44, 50, 59, 41,\n",
       "         51, 53, 60, 63, 53, 58, 46, 40, 55, 66, 66, 54, 64, 42, 55, 74, 55, 66,\n",
       "         69, 35, 87, 40, 59, 55, 63, 37, 21, 44, 54, 35, 37, 53, 58, 84, 52, 46,\n",
       "         52, 65, 66, 55, 60, 86, 51, 33, 54, 62, 28, 60, 94, 61, 45, 42, 51, 42,\n",
       "         45, 59, 39, 63, 56, 46, 39, 72, 55, 94, 59, 45, 55, 64, 48, 67, 51, 25,\n",
       "         72, 52, 52, 83, 41, 59, 77, 60, 58, 52, 55, 31, 65, 70, 34, 45, 71, 70,\n",
       "         56, 96, 30, 99, 47, 45, 64, 57, 45, 40, 62, 68, 94, 91, 47, 48, 99, 41,\n",
       "         54, 68, 53, 72, 64, 62, 51, 89, 62, 70, 77, 43, 33, 61, 60, 42, 60, 42,\n",
       "         81, 63, 44, 74, 48, 51, 58, 66, 45, 64, 63, 47, 67, 55, 62, 65, 71, 52,\n",
       "         36, 29, 44, 43, 69, 41, 45, 53, 59, 92, 72, 64, 49, 46, 58, 41, 60, 72,\n",
       "         67, 70, 37, 59, 70, 65, 59, 52, 59, 65, 63, 64, 41, 60, 62, 54, 55, 47,\n",
       "         69, 57, 41, 29, 86, 49, 58, 45, 37, 70, 57, 62, 51, 64, 41, 49, 48, 52,\n",
       "         78, 56, 48, 81, 68, 64, 68]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b449784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=10, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=10, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "306151be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35, 29,  5, 18,  5,  5, 17,  5, 26, 30,  5, 16,  5,  5,  5, 26,  8,  5,\n",
      "         15,  4,  5, 14,  5,  5,  5,  5,  5, 14,  2, 15, 25,  5,  5, 16, 25,  4,\n",
      "          5,  4,  5, 17,  5,  5, 18, 29, 35, 35, 35, 30, 18, 35, 35, 30, 26, 35,\n",
      "         25, 35, 35, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [35, 29,  5, 18,  5,  5, 26, 30, 17, 35, 35, 35, 26, 30, 16, 35, 35, 35,\n",
      "         35, 35, 16, 25, 35, 35, 17, 25,  5, 26, 42, 25,  5,  5, 18, 30,  5, 18,\n",
      "          4,  5,  5,  5, 26,  8,  5, 17,  5, 26,  8,  5, 16,  5,  5,  5, 26, 29,\n",
      "         35, 25,  5, 26, 35, 26, 22, 29, 25, 30,  5, 15,  5, 26, 34, 25,  5,  5,\n",
      "          5,  5, 15, 34, 25,  5, 16, 25,  4,  5, 16,  5,  5,  5,  5,  4, 17, 16,\n",
      "         25,  4, 18],\n",
      "        [35, 35, 35, 35, 30, 26, 35,  5, 18,  5,  5, 26, 43, 25,  5,  5, 26, 43,\n",
      "         25,  5, 18, 29, 25, 35, 26, 22, 27, 25, 30,  5, 18,  5,  5,  5,  5,  5,\n",
      "         18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [35, 35, 35, 26, 22, 29, 25, 30,  5, 18,  5,  5, 17,  5, 26, 30,  5, 16,\n",
      "          5,  5,  5, 15, 24,  4, 33, 23,  5,  5,  5, 15,  5, 16, 25,  4,  5,  4,\n",
      "          5, 17,  5,  5, 18, 29, 35,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [30,  5, 18,  5,  5,  5,  5,  5, 18, 30, 35, 26, 22, 29, 25, 35, 22, 35,\n",
      "          5, 18,  5,  5,  5, 26,  8,  5, 17,  5,  5,  5, 16,  4,  5,  4,  5, 26,\n",
      "         30,  5, 15,  5,  5,  5, 26, 29, 35,  5, 14,  5,  5,  5,  5, 26, 34, 25,\n",
      "          5, 14, 25,  5, 26, 42, 25,  5, 15, 25,  5, 16,  5, 17, 25,  3, 18,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [35,  5, 18,  5,  5, 26, 35, 25,  5, 17,  3,  5, 26, 30,  5, 16,  5,  5,\n",
      "          5, 26,  8,  5, 15,  4,  4, 26, 24, 35, 33, 23, 14, 35, 35, 24, 35, 33,\n",
      "         23, 26, 30, 13, 35, 35, 30, 26, 35, 25, 35, 35, 13, 25, 35, 35, 14, 25,\n",
      "          5, 14,  4,  5,  4,  5, 26, 30, 25,  5, 15, 14, 25,  5,  5, 16, 34, 25,\n",
      "          4,  5, 17,  5, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [35, 35, 29,  5, 18,  5,  5, 17,  4,  5,  5, 26, 35, 21, 30, 25,  5, 26,\n",
      "         30,  5, 16,  5,  5,  5, 26, 29, 35,  5, 15,  5,  5,  5,  5,  5, 15, 25,\n",
      "          5, 26, 42, 25,  5, 16, 25,  5, 17,  5,  5, 18, 30, 35, 26, 22, 29, 25,\n",
      "         35, 22, 35, 35, 30, 18, 35, 26, 35, 25, 35, 35, 35, 35, 18, 35,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [35,  5, 18,  5, 26, 30, 35, 26, 22, 29, 25, 29, 35, 24, 35, 33, 23, 17,\n",
      "         35, 29, 35, 35, 30, 17, 25,  5,  4, 17,  4,  5,  4,  5, 26, 30,  5, 16,\n",
      "          5,  5,  5, 15,  5, 26,  5,  4,  4, 15, 35,  5, 15,  5,  5,  5,  5, 26,\n",
      "         34, 25,  5, 15, 25,  5, 16, 25,  5, 18, 17,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [35, 30, 26, 35, 25, 35, 35, 22, 35, 35, 26, 22, 29, 25, 30,  5, 18,  5,\n",
      "          5, 17,  5, 26, 30,  5, 16,  5,  5,  5, 26, 29,  5, 15,  5,  5,  5,  5,\n",
      "          5, 15, 25,  5,  5, 16, 25,  4,  5,  4,  5, 17,  5,  4, 18,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [30,  5, 18,  5,  5,  5,  5,  5, 18, 30, 35, 26, 22, 29, 25, 35, 22, 35,\n",
      "          5, 18,  5,  5,  5, 26,  8,  5, 17,  5,  5,  5, 16,  4,  5,  4,  5, 26,\n",
      "         30,  5, 15,  5,  5,  5, 26, 29, 35,  5, 14,  5,  5,  5,  5, 26, 34, 25,\n",
      "          5, 14, 25,  5, 26, 42, 25,  5, 15, 25,  5, 16,  5, 17, 25,  5,  5, 18,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0]])\n",
      "tensor([2, 0, 0, 1, 2, 0, 1, 2, 2, 2])\n",
      "tensor([58, 93, 37, 43, 71, 77, 70, 65, 51, 72])\n",
      "torch.Size([10, 93])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(test_dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0b8404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size,\n",
    "                 num_layers, fc_hidden_size, output_size):\n",
    "        super(). __init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim (embedding))\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, num_layers,\n",
    "                           batch_first=True, bidirectional=True) \n",
    "        # only use the hidden_size, since its a many to one\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size * num_layers, fc_hidden_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc_hidden_size, output_size) \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        # text_length = [batch size]\n",
    "        \n",
    "        # [sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        # pack sequence to avoid using <paddings> during computations (saves computations)\n",
    "        # lengths needs to be on cpu\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, text_lengths.to('cpu'),\n",
    "                                                enforce_sorted=False, batch_first=True)\n",
    "        # =>  [batch_size, sentence len,  embedding dim]\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        out, (hidden, cell) = self.rnn(out) ## lstm with input, hidden, and internal (cell) state\n",
    "        # out dim: [batch size, sentence length, hidden dim]\n",
    "        # cell dim: [num layers, batch size, hidden dim]\n",
    "        # hidden dim: [num_layers, batch_size, hidden dim]\n",
    "        \n",
    "        # final layer foward hidden state     \n",
    "        hidden_fwd = hidden[-2]\n",
    "        # final layer backwaed hidden state \n",
    "        hidden_bck = hidden[-1]\n",
    "        \n",
    "        # concatenate the 2 layers to pass to linear layer\n",
    "        # hidden_fwd/bck = [batch size, hid dim]\n",
    "        out = torch.cat((hidden_fwd, hidden_bck), dim = 1)\n",
    "        # out = [batch size, hid dim * 2]\n",
    "\n",
    "        out = self.fc1(out) ## first dense layer\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out) ## final layer\n",
    "        out = self.sigmoid(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643a128",
   "metadata": {},
   "source": [
    "Here, I'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "    vocab_size: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "    output_size: Size of our desired output; the number of class scores we want to output.\n",
    "    embedding_dim: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "    num_filters: Number of filters that each convolutional layer produces as output.\n",
    "    filter_sizes: A list of kernel sizes; one convolutional layer will be created for each kernel size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a426fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab1) ## len of vocab size\n",
    "embed_dim = 70 ## input size, usually around 50-500 dimensions\n",
    "num_layers = 2 ## number of recurrent layers, 2 would mean stacking 2 layers to form stacked LSTM\n",
    "rnn_hidden_size = 100 ## usually around 100-500 dimensions\n",
    "fc_hidden_size = 100 ## usually around 100-500 dimensions\n",
    "output_size = len(df.activity.unique()) ## num_classes 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90638f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will instantiate the class LSTM1 object.\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size,\n",
    "            num_layers, fc_hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91feee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(45, 70, padding_idx=0)\n",
       "  (rnn): LSTM(70, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f744f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    # model training mode (gradient computation)\n",
    "    model.train()\n",
    "    # initailiz acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    for idx, (text, label, length) in enumerate (dataloader):\n",
    "        # reset gradients to zero before each instance\n",
    "        optimizer.zero_grad()\n",
    "        # label predictions (forward papagation)\n",
    "        # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "        predicted_label = model(text, length).squeeze(1) # or [:,0]\n",
    "        # loss calculation\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        # compute gradients (backward propagation) \n",
    "        # to minimize loss functions with gradient descent\n",
    "        loss.backward()\n",
    "        # update parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "        # logging\n",
    "        if not idx % 50:\n",
    "            print(f\"Epoch: {epoch + 1:04d}/{num_epochs:0d} | \"\n",
    "                  f\"Batch {idx:03d}/{len(dataloader):03d} | \"\n",
    "                  f\"Loss: {loss:.4f}\")\n",
    "        # compute total accuracy\n",
    "        # return an indice of the max value of all elements\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        # compute total loss after back prop and parameter update\n",
    "        total_loss += loss.item() * label.size(0)\n",
    "    # compare true labels with the predicted labels to compute accuracy\n",
    "    return total_acc/len(dataloader.dataset), \\\n",
    "            total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b6545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    # model evaluation mode (no gradient computation)\n",
    "    model.eval()\n",
    "    # initailize acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    # disabling gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for text, label, length in dataloader:\n",
    "            # label predictions (forward papagation)\n",
    "            # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "            predicted_label = model(text, length).squeeze(1) # reshape\n",
    "            # loss calculation\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            # compute total accuracy\n",
    "            # return an indice of the max value of all elements\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            # compute total loss after back prop and parameter update\n",
    "            total_loss += loss.item() * label.size(0)\n",
    "        # compare true labels with the predicted labels to compute accuracy\n",
    "        return total_acc/len(dataloader.dataset), \\\n",
    "                total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ef067ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiclass classification we use\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Adam Optimizer to update parameters based on the computed gradients\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5ed3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c632c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/10 | Batch 000/208 | Loss: 1.1002\n",
      "Epoch: 0001/10 | Batch 050/208 | Loss: 1.0924\n",
      "Epoch: 0001/10 | Batch 100/208 | Loss: 1.0496\n",
      "Epoch: 0001/10 | Batch 150/208 | Loss: 0.9923\n",
      "Epoch: 0001/10 | Batch 200/208 | Loss: 1.0882\n",
      "Train Acc.: 51.23%\n",
      "Valid Acc.: 59.85%\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 0002/10 | Batch 000/208 | Loss: 0.9923\n",
      "Epoch: 0002/10 | Batch 050/208 | Loss: 1.0715\n",
      "Epoch: 0002/10 | Batch 100/208 | Loss: 1.0009\n",
      "Epoch: 0002/10 | Batch 150/208 | Loss: 0.8059\n",
      "Epoch: 0002/10 | Batch 200/208 | Loss: 1.0082\n",
      "Train Acc.: 58.36%\n",
      "Valid Acc.: 60.62%\n",
      "Time elapsed: 1.19 min\n",
      "Epoch: 0003/10 | Batch 000/208 | Loss: 1.1499\n",
      "Epoch: 0003/10 | Batch 050/208 | Loss: 0.8551\n",
      "Epoch: 0003/10 | Batch 100/208 | Loss: 0.8107\n",
      "Epoch: 0003/10 | Batch 150/208 | Loss: 0.6906\n",
      "Epoch: 0003/10 | Batch 200/208 | Loss: 0.8300\n",
      "Train Acc.: 60.72%\n",
      "Valid Acc.: 62.93%\n",
      "Time elapsed: 1.80 min\n",
      "Epoch: 0004/10 | Batch 000/208 | Loss: 0.8862\n",
      "Epoch: 0004/10 | Batch 050/208 | Loss: 1.1139\n",
      "Epoch: 0004/10 | Batch 100/208 | Loss: 0.8767\n",
      "Epoch: 0004/10 | Batch 150/208 | Loss: 0.7629\n",
      "Epoch: 0004/10 | Batch 200/208 | Loss: 0.9827\n",
      "Train Acc.: 62.80%\n",
      "Valid Acc.: 64.09%\n",
      "Time elapsed: 2.40 min\n",
      "Epoch: 0005/10 | Batch 000/208 | Loss: 0.7961\n",
      "Epoch: 0005/10 | Batch 050/208 | Loss: 0.9510\n",
      "Epoch: 0005/10 | Batch 100/208 | Loss: 0.6581\n",
      "Epoch: 0005/10 | Batch 150/208 | Loss: 0.8631\n",
      "Epoch: 0005/10 | Batch 200/208 | Loss: 0.9341\n",
      "Train Acc.: 63.57%\n",
      "Valid Acc.: 61.00%\n",
      "Time elapsed: 2.99 min\n",
      "Epoch: 0006/10 | Batch 000/208 | Loss: 0.9384\n",
      "Epoch: 0006/10 | Batch 050/208 | Loss: 1.2261\n",
      "Epoch: 0006/10 | Batch 100/208 | Loss: 0.8297\n",
      "Epoch: 0006/10 | Batch 150/208 | Loss: 1.1544\n",
      "Epoch: 0006/10 | Batch 200/208 | Loss: 1.0711\n",
      "Train Acc.: 63.90%\n",
      "Valid Acc.: 66.02%\n",
      "Time elapsed: 3.60 min\n",
      "Epoch: 0007/10 | Batch 000/208 | Loss: 0.8658\n",
      "Epoch: 0007/10 | Batch 050/208 | Loss: 0.6916\n",
      "Epoch: 0007/10 | Batch 100/208 | Loss: 0.7908\n",
      "Epoch: 0007/10 | Batch 150/208 | Loss: 0.9467\n",
      "Epoch: 0007/10 | Batch 200/208 | Loss: 1.0180\n",
      "Train Acc.: 66.89%\n",
      "Valid Acc.: 64.48%\n",
      "Time elapsed: 4.19 min\n",
      "Epoch: 0008/10 | Batch 000/208 | Loss: 0.9661\n",
      "Epoch: 0008/10 | Batch 050/208 | Loss: 0.8099\n",
      "Epoch: 0008/10 | Batch 100/208 | Loss: 0.8563\n",
      "Epoch: 0008/10 | Batch 150/208 | Loss: 1.0282\n",
      "Epoch: 0008/10 | Batch 200/208 | Loss: 0.9585\n",
      "Train Acc.: 67.71%\n",
      "Valid Acc.: 65.64%\n",
      "Time elapsed: 4.81 min\n",
      "Epoch: 0009/10 | Batch 000/208 | Loss: 0.8192\n",
      "Epoch: 0009/10 | Batch 050/208 | Loss: 0.9944\n",
      "Epoch: 0009/10 | Batch 100/208 | Loss: 0.8996\n",
      "Epoch: 0009/10 | Batch 150/208 | Loss: 0.7990\n",
      "Epoch: 0009/10 | Batch 200/208 | Loss: 0.7166\n",
      "Train Acc.: 69.49%\n",
      "Valid Acc.: 69.88%\n",
      "Time elapsed: 5.42 min\n",
      "Epoch: 0010/10 | Batch 000/208 | Loss: 0.6363\n",
      "Epoch: 0010/10 | Batch 050/208 | Loss: 1.0996\n",
      "Epoch: 0010/10 | Batch 100/208 | Loss: 0.7267\n",
      "Epoch: 0010/10 | Batch 150/208 | Loss: 0.5753\n",
      "Epoch: 0010/10 | Batch 200/208 | Loss: 0.8266\n",
      "Train Acc.: 70.27%\n",
      "Valid Acc.: 66.41%\n",
      "Time elapsed: 5.97 min\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dataloader)\n",
    "    acc_val , loss_val = evaluate(val_dataloader)\n",
    "    print(f\"Train Acc.: {100 * acc_train:.2f}%\"\n",
    "          f\"\\nValid Acc.: {100 * acc_val:.2f}%\")\n",
    "    print(f'Time elapsed: {(time.time() - start_time) / 60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcfec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d524686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc.: 68.73%\n"
     ]
    }
   ],
   "source": [
    "acc_test, _ = evaluate(test_dataloader)\n",
    "print(f\"Test Acc.: {100 * acc_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061864a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ed491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc79ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only difference here is that instead of using a sigmoid function to squash the input between 0 and 1,\n",
    "# we use the argmax to get the highest predicted class index. \n",
    "# or we use sofmax activation function and squash the input to range 0 and 1 and sum them to 1\n",
    "# in this case we get both the class label pred and the predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7d29ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2_lstm.ipynb\n",
    "\n",
    "sentiment_label = {0: \"low\",\n",
    "                   1: \"medium\",\n",
    "                   2: \"high\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    # evaluation mode (no gradients)\n",
    "    with torch.no_grad():\n",
    "        # tokenize and index the tokens\n",
    "        processed_text = torch.tensor(text_pipeline(text))\n",
    "        # add a batch dimension\n",
    "        processed_text = processed_text.unsqueeze(0).to(device)\n",
    "        # compute sequence length\n",
    "        text_length = processed_text.size(0)\n",
    "        # convert to tensor and add a batch dimension\n",
    "        text_length = torch.tensor(text_length).unsqueeze(0)\n",
    "        # prediction\n",
    "        prediction = model(processed_text, text_length)\n",
    "        # reduction real numbers to values between 0 and 1\n",
    "        probability = torch.softmax(prediction, dim=1)\n",
    "        # get the max value of all elements\n",
    "        predicted_probability, predicted_class, = torch.max(probability, dim=1)\n",
    "        # convert tensor holding a single value into an integer\n",
    "        return predicted_class.item(), predicted_probability.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8797b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2 = high\n",
      "Probability: 0.5652756690979004\n"
     ]
    }
   ],
   "source": [
    "text = \"O=C(C#CCCN1CCOCC1)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncn\"\n",
    "pred_class, pred_proba = predict(text, text_pipeline)\n",
    "\n",
    "print(f'Predicted Class: {pred_class} = {sentiment_label[pred_class]}')\n",
    "print(f'Probability: {pred_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b042e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0 = low\n",
      "Probability: 0.4145854711532593\n"
     ]
    }
   ],
   "source": [
    "text = \"Nc1ncnc2c1ncn2[C@@H]1O[C@H](CO)[C@@H](O)[C@H]1O\"\n",
    "pred_class, pred_proba = predict(text, text_pipeline)\n",
    "\n",
    "print(f'Predicted Class: {pred_class} = {sentiment_label[pred_class]}')\n",
    "print(f'Probability: {pred_proba}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
